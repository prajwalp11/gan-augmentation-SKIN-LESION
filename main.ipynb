{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, channels):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0, bias=False),#seecond changed to 7 from 4\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "# Define the discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(channels, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),  # Changed output channels to 1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the GAN\n",
    "latent_dim = 100\n",
    "channels = 3  # RGB channels\n",
    "img_size = 64  # Desired image size    64 to 224 change\n",
    "\n",
    "# Initialize the generator and discriminator\n",
    "generator = Generator(latent_dim, channels)\n",
    "discriminator = Discriminator(channels)\n",
    "\n",
    "# Set up the loss function and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SkinLesionDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.png') or f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# Load your skin lesion dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*channels, [0.5]*channels)\n",
    "])\n",
    "\n",
    "dataset = SkinLesionDataset('images', transform=transform)\n",
    "total_images = len(dataset)\n",
    "\n",
    "# Calculate the batch size\n",
    "batch_size = min(total_images, 64)  # Use 64 as the maximum batch size\n",
    "\n",
    "# Create the data loader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], d_loss: 0.02593955211341381, g_loss: 6.403222560882568\n",
      "Epoch [2/100], d_loss: 0.007860735058784485, g_loss: 6.515342712402344\n",
      "Epoch [3/100], d_loss: 0.015968013554811478, g_loss: 6.424894332885742\n",
      "Epoch [4/100], d_loss: 0.0032134975772351027, g_loss: 6.879193305969238\n",
      "Epoch [5/100], d_loss: 0.008223801851272583, g_loss: 6.21870231628418\n",
      "Epoch [6/100], d_loss: 0.006766670383512974, g_loss: 6.731940746307373\n",
      "Epoch [7/100], d_loss: 0.0075263953767716885, g_loss: 6.514240741729736\n",
      "Epoch [8/100], d_loss: 0.01137425284832716, g_loss: 6.28379487991333\n",
      "Epoch [9/100], d_loss: 0.005279887467622757, g_loss: 6.377618789672852\n",
      "Epoch [10/100], d_loss: 0.010681990534067154, g_loss: 6.394758701324463\n",
      "Epoch [11/100], d_loss: 0.011862666346132755, g_loss: 5.996876239776611\n",
      "Epoch [12/100], d_loss: 0.00313474889844656, g_loss: 6.748960494995117\n",
      "Epoch [13/100], d_loss: 0.009139335714280605, g_loss: 6.2197346687316895\n",
      "Epoch [14/100], d_loss: 0.006484953220933676, g_loss: 6.692654609680176\n",
      "Epoch [15/100], d_loss: 0.0038019795902073383, g_loss: 6.476572036743164\n",
      "Epoch [16/100], d_loss: 0.00928625836968422, g_loss: 5.99937105178833\n",
      "Epoch [17/100], d_loss: 0.0017647340428084135, g_loss: 6.841041088104248\n",
      "Epoch [18/100], d_loss: 0.01073373295366764, g_loss: 6.615006923675537\n",
      "Epoch [19/100], d_loss: 0.006393021903932095, g_loss: 7.1977691650390625\n",
      "Epoch [20/100], d_loss: 0.004267057403922081, g_loss: 6.363017559051514\n",
      "Epoch [21/100], d_loss: 0.004829790908843279, g_loss: 6.362381458282471\n",
      "Epoch [22/100], d_loss: 0.00687694177031517, g_loss: 6.1029052734375\n",
      "Epoch [23/100], d_loss: 0.008193966932594776, g_loss: 6.437589645385742\n",
      "Epoch [24/100], d_loss: 0.011850330047309399, g_loss: 6.842676162719727\n",
      "Epoch [25/100], d_loss: 0.007513945922255516, g_loss: 6.245357990264893\n",
      "Epoch [26/100], d_loss: 0.006941413506865501, g_loss: 6.252954006195068\n",
      "Epoch [27/100], d_loss: 0.011768518947064877, g_loss: 5.913583278656006\n",
      "Epoch [28/100], d_loss: 0.005794806405901909, g_loss: 6.848188877105713\n",
      "Epoch [29/100], d_loss: 0.012407274916768074, g_loss: 5.640048027038574\n",
      "Epoch [30/100], d_loss: 0.008002282120287418, g_loss: 6.3277997970581055\n",
      "Epoch [31/100], d_loss: 0.009832462295889854, g_loss: 6.372082233428955\n",
      "Epoch [32/100], d_loss: 0.013608735986053944, g_loss: 7.093289375305176\n",
      "Epoch [33/100], d_loss: 0.013954084366559982, g_loss: 6.390554904937744\n",
      "Epoch [34/100], d_loss: 0.013201896101236343, g_loss: 6.446072101593018\n",
      "Epoch [35/100], d_loss: 0.3105199933052063, g_loss: 0.9709144234657288\n",
      "Epoch [36/100], d_loss: 7.442906379699707, g_loss: 12.051224708557129\n",
      "Epoch [37/100], d_loss: 0.17785698175430298, g_loss: 3.696408987045288\n",
      "Epoch [38/100], d_loss: 0.1314789354801178, g_loss: 5.6448798179626465\n",
      "Epoch [39/100], d_loss: 0.3245522975921631, g_loss: 4.0285868644714355\n",
      "Epoch [40/100], d_loss: 0.2373410165309906, g_loss: 4.410799503326416\n",
      "Epoch [41/100], d_loss: 0.2572011351585388, g_loss: 3.4427030086517334\n",
      "Epoch [42/100], d_loss: 0.14431294798851013, g_loss: 5.697932720184326\n",
      "Epoch [43/100], d_loss: 0.061343275010585785, g_loss: 6.24021577835083\n",
      "Epoch [44/100], d_loss: 0.14428789913654327, g_loss: 5.633387088775635\n",
      "Epoch [45/100], d_loss: 0.3011838495731354, g_loss: 9.254907608032227\n",
      "Epoch [46/100], d_loss: 0.05862241983413696, g_loss: 7.098369121551514\n",
      "Epoch [47/100], d_loss: 0.07333601266145706, g_loss: 4.73347282409668\n",
      "Epoch [48/100], d_loss: 0.15507730841636658, g_loss: 5.201212406158447\n",
      "Epoch [49/100], d_loss: 1.0463712215423584, g_loss: 13.152759552001953\n",
      "Epoch [50/100], d_loss: 0.35459470748901367, g_loss: 2.0208470821380615\n",
      "Epoch [51/100], d_loss: 0.22690126299858093, g_loss: 4.05433464050293\n",
      "Epoch [52/100], d_loss: 0.28822267055511475, g_loss: 4.320069789886475\n",
      "Epoch [53/100], d_loss: 0.21740645170211792, g_loss: 3.1039934158325195\n",
      "Epoch [54/100], d_loss: 0.1964060664176941, g_loss: 3.1492156982421875\n",
      "Epoch [55/100], d_loss: 0.11210691928863525, g_loss: 4.202181816101074\n",
      "Epoch [56/100], d_loss: 0.153199702501297, g_loss: 4.161419868469238\n",
      "Epoch [57/100], d_loss: 0.12915226817131042, g_loss: 4.657650470733643\n",
      "Epoch [58/100], d_loss: 0.22212621569633484, g_loss: 4.9922380447387695\n",
      "Epoch [59/100], d_loss: 0.14528922736644745, g_loss: 3.8572614192962646\n",
      "Epoch [60/100], d_loss: 0.18268215656280518, g_loss: 4.977792263031006\n",
      "Epoch [61/100], d_loss: 1.0244547128677368, g_loss: 0.5811886191368103\n",
      "Epoch [62/100], d_loss: 0.18282002210617065, g_loss: 3.984622001647949\n",
      "Epoch [63/100], d_loss: 0.20132283866405487, g_loss: 3.6379494667053223\n",
      "Epoch [64/100], d_loss: 0.4779347777366638, g_loss: 4.632248401641846\n",
      "Epoch [65/100], d_loss: 0.48096805810928345, g_loss: 3.7688143253326416\n",
      "Epoch [66/100], d_loss: 0.4642850160598755, g_loss: 5.167422771453857\n",
      "Epoch [67/100], d_loss: 0.2229759693145752, g_loss: 3.45585036277771\n",
      "Epoch [68/100], d_loss: 0.1622532606124878, g_loss: 3.9966866970062256\n",
      "Epoch [69/100], d_loss: 0.35981491208076477, g_loss: 5.941529750823975\n",
      "Epoch [70/100], d_loss: 0.39847850799560547, g_loss: 3.573240041732788\n",
      "Epoch [71/100], d_loss: 0.3866000771522522, g_loss: 1.9083997011184692\n",
      "Epoch [72/100], d_loss: 0.9784425497055054, g_loss: 0.2871515452861786\n",
      "Epoch [73/100], d_loss: 0.750455915927887, g_loss: 2.162308692932129\n",
      "Epoch [74/100], d_loss: 0.5444244146347046, g_loss: 2.6901369094848633\n",
      "Epoch [75/100], d_loss: 0.5413148403167725, g_loss: 1.8720980882644653\n",
      "Epoch [76/100], d_loss: 0.42215675115585327, g_loss: 1.916911005973816\n",
      "Epoch [77/100], d_loss: 0.4767065942287445, g_loss: 2.013587713241577\n",
      "Epoch [78/100], d_loss: 0.6152253150939941, g_loss: 0.9595978260040283\n",
      "Epoch [79/100], d_loss: 0.30524685978889465, g_loss: 2.2312934398651123\n",
      "Epoch [80/100], d_loss: 0.4885793924331665, g_loss: 2.1171200275421143\n",
      "Epoch [81/100], d_loss: 0.41977229714393616, g_loss: 4.564905166625977\n",
      "Epoch [82/100], d_loss: 0.3264622688293457, g_loss: 2.9109511375427246\n",
      "Epoch [83/100], d_loss: 0.7371203303337097, g_loss: 1.128047227859497\n",
      "Epoch [84/100], d_loss: 0.43338674306869507, g_loss: 2.3039963245391846\n",
      "Epoch [85/100], d_loss: 0.4013047218322754, g_loss: 2.7991297245025635\n",
      "Epoch [86/100], d_loss: 0.9402267932891846, g_loss: 6.0869245529174805\n",
      "Epoch [87/100], d_loss: 0.7336333990097046, g_loss: 2.6820809841156006\n",
      "Epoch [88/100], d_loss: 0.47272372245788574, g_loss: 2.2959325313568115\n",
      "Epoch [89/100], d_loss: 0.551852822303772, g_loss: 4.523532390594482\n",
      "Epoch [90/100], d_loss: 0.8130038380622864, g_loss: 5.605453014373779\n",
      "Epoch [91/100], d_loss: 0.32032468914985657, g_loss: 3.97896409034729\n",
      "Epoch [92/100], d_loss: 0.455422967672348, g_loss: 4.566970348358154\n",
      "Epoch [93/100], d_loss: 0.8467929363250732, g_loss: 1.8791273832321167\n",
      "Epoch [94/100], d_loss: 0.22691598534584045, g_loss: 4.766841888427734\n",
      "Epoch [95/100], d_loss: 0.16290009021759033, g_loss: 3.8330862522125244\n",
      "Epoch [96/100], d_loss: 0.22093063592910767, g_loss: 3.633120536804199\n",
      "Epoch [97/100], d_loss: 0.29909756779670715, g_loss: 1.6111136674880981\n",
      "Epoch [98/100], d_loss: 1.4958539009094238, g_loss: 0.3734769821166992\n",
      "Epoch [99/100], d_loss: 1.0383931398391724, g_loss: 1.5994830131530762\n",
      "Epoch [100/100], d_loss: 0.5408129096031189, g_loss: 2.654528856277466\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for real_images in dataloader:\n",
    "        # Train the discriminator\n",
    "        d_optimizer.zero_grad()\n",
    "        batch_size = real_images.size(0)\n",
    "        real_labels = torch.ones(batch_size, 1, 1, 1)\n",
    "        fake_labels = torch.zeros(batch_size, 1, 1, 1)\n",
    "        real_outputs = discriminator(real_images)\n",
    "        real_loss = criterion(real_outputs, real_labels)\n",
    "        latent_vectors = torch.randn(batch_size, latent_dim, 1, 1)\n",
    "        fake_images = generator(latent_vectors)\n",
    "        fake_outputs = discriminator(fake_images.detach())\n",
    "        fake_loss = criterion(fake_outputs, fake_labels)\n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Train the generator\n",
    "        g_optimizer.zero_grad()\n",
    "        latent_vectors = torch.randn(batch_size, latent_dim, 1, 1)\n",
    "        fake_images = generator(latent_vectors)\n",
    "        fake_outputs = discriminator(fake_images)\n",
    "        g_loss = criterion(fake_outputs, real_labels)  # Use real labels for generator loss\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "    # Print losses and save generated images\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], d_loss: {d_loss.item()}, g_loss: {g_loss.item()}')\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            latent_vectors = torch.randn(16, latent_dim, 1, 1)\n",
    "            fake_images = generator(latent_vectors)\n",
    "            image_path = os.path.join('gan_images', f'generated_image_epoch_{epoch+1}.png')\n",
    "            save_image(fake_images, image_path, normalize=True)\n",
    "\n",
    "# Save the trained generator model\n",
    "torch.save(generator.state_dict(), 'generator.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
